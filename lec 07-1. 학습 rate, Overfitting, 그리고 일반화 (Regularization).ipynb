{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent\n",
    "# step: -a Δα(w1,w2)\n",
    "#         Δα(w1,w2): deaivative\n",
    "# a: learning_rate = 0.001\n",
    "\n",
    "# 1. learning_rate\n",
    "# Large learing rate: overshooting\n",
    "#: 너무 크게 잡으면 값이 숫자가 아닌 값이 나오거나 밖으로 튕겨나갈 수 있음.\n",
    "# 반대로 너무 작게 잡으면\n",
    "# Small learing rate: takes too long, stops at local minimum\n",
    "# => Try several learning rates\n",
    "# 보통 0.01로 잡고 상황, 환경에 따라 변경\n",
    "# observe the cost function\n",
    "# check it goes down in a reasonable rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data(x) preprocessing for gradient gradient descent\n",
    "# 지난 그래프에서 가로(w), 세로(cost함수)\n",
    "# w(weight)가 2개 있어서 가로(w1), 세로(w2)\n",
    "# 이차원에서 등고선처럼 표시: 낮은 지점에 도착하는 것이 목표\n",
    "\n",
    "# ex:    x1     x2      y\n",
    "#         1    9000     A\n",
    "#         2   -5000     A\n",
    "#         4   -2000     B\n",
    "#         6    8000     B\n",
    "#         9    9000     C\n",
    "# 간격이 작아 a값이 좋더라도 잘못해 밖으로 나가게 되면 \n",
    "# 튀어나가게 된다. 그래서 주의!\n",
    "# 데이터 값의 차이가 큰 경우에는 normalized할 필요가 있다.\n",
    "\n",
    "# original data, zero-centered data, normalized data\n",
    "# zero-centered data: 데이터의 중심이 0으로 갈수 있도록 바꾸어 주는 방법\n",
    "# normalized data: 값 전체의 범위가 항상 어떤 범위안에 들어가도록 해주는 방법 \n",
    "\n",
    "# 하는 방법\n",
    "# Standardization: xj' = (xj-mj)/oj    # mj: 평균, oj: 분산\n",
    "# X_std[:,0] = (X[:,0] - X[:,0].mean()) / X[;,0].std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. 문제: Overfitting\n",
    "# 1) our model is very goood with training data(학습데이터) set (with memorization)\n",
    "# 2) Not good at test dataset or in real use\n",
    "# 학습데이터에 맞는 모델을 만들어냈지만\n",
    "# 실제 사용(or test)하면 안맞는 경우\n",
    "\n",
    "# ex: 실제데이터가 +,-를 구분, 예측하는 형태의 문제\n",
    "# model2는 갖고 있는 학습데이터에는 잘 맞겠지만 다른 데이터에는 잘 안맞을 수 있음.\n",
    "\n",
    "# solutions for overfitting\n",
    "# 1) more training data\n",
    "# 2) reduce the number of features: 중복을 줄임\n",
    "# 3) Regularization(일반화시킨다.)\n",
    "# : let's not have too big numbers in the weight: 너무 큰 값을 가지지 말자.\n",
    "# : 피자: weight 값을 작게 하자 <-> 구부리자: weight 값을 크게 하자\n",
    "# cost를 최소화시키는 목표에      # h: 란다 대신으로 표시\n",
    "#                                    :regularization strength\n",
    "# Loss = 1/N (시그마 i: training set) D(s(wxi+b),Li) + h(시그마) W^2\n",
    "# 최소화시킴 -> w^2의 값을 작아지게 만들어 펴는 것\n",
    "\n",
    "# l2reg = 0.001 * tf.reduce_sum(tf.square(w))\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
